{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d1d9d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Enter your search query: lauda\n",
      "Result 1: 'This document is the second document.'\n",
      "Score: 0.2796\n",
      "\n",
      "Result 2: 'Is this the first document?'\n",
      "Score: 0.2341\n",
      "\n",
      "Result 3: 'This is the first document.'\n",
      "Score: 0.2269\n",
      "\n",
      "Result 4: 'And this is the third one.'\n",
      "Score: 0.1302\n",
      "\n",
      "This document is the second document.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# load pre-trained model\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# define corpus\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# encode corpus into vectors\n",
    "corpus_vectors = [nlp(doc).vector for doc in corpus]\n",
    "\n",
    "# get user input\n",
    "user_query = input(\"Enter your search query: \")\n",
    "\n",
    "# encode user query into a vector\n",
    "query_vector = nlp(user_query).vector\n",
    "\n",
    "# compute similarity scores between query and corpus vectors\n",
    "similarity_scores = cosine_similarity([query_vector], corpus_vectors)[0]\n",
    "\n",
    "# get indices of top 5 most similar documents\n",
    "top_indices = similarity_scores.argsort()[-5:][::-1]\n",
    "\n",
    "# print top 5 most similar documents and their scores\n",
    "for i, index in enumerate(top_indices):\n",
    "    print(f\"Result {i+1}: '{corpus[index]}'\\nScore: {similarity_scores[index]:.4f}\\n\")\n",
    "    \n",
    "most_similar_index = similarity_scores.argmax()\n",
    "\n",
    "# Print the most similar document\n",
    "print(corpus[most_similar_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b1e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687b64ac235748c0927bc320c3017026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# load pre-trained BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# define a function to encode a preprocessed document as a vector\n",
    "def encode_document(document):\n",
    "    # tokenize the preprocessed document\n",
    "    tokens = tokenizer(document, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # encode the tokens using the pre-trained BERT model\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        vector = output.pooler_output\n",
    "    return vector\n",
    "\n",
    "# encode each preprocessed document as a vector and add it to the Pinecone index\n",
    "for i, preprocessed_document in enumerate(preprocessed_documents):\n",
    "    vector = encode_document(preprocessed_document)\n",
    "    pinecone.upsert_index(index_name, [i], [vector])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eab3f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# initialize Pinecone client\n",
    "pinecone.init(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "# create a Pinecone index for text documents\n",
    "index_name = \"text_index\"\n",
    "pinecone.create_index(index_name, {\"metric\": \"cosine\"})\n",
    "\n",
    "# define a list of documents to index\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A quick brown dog jumps over the lazy cat\",\n",
    "    \"The lazy dog is quick to bark but slow to bite\",\n",
    "    \"The cat is lazy and likes to sleep all day\",\n",
    "]\n",
    "\n",
    "# define a function to preprocess a document (i.e., tokenize and remove stop words)\n",
    "def preprocess_document(document):\n",
    "    stop_words = {\"the\", \"a\", \"an\", \"is\", \"and\", \"but\", \"to\"}\n",
    "    tokens = document.lower().split()\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# preprocess each document and store the preprocessed documents in a list\n",
    "preprocessed_documents = [preprocess_document(document) for document in documents]\n",
    "\n",
    "# define a function to encode a preprocessed document as a vector\n",
    "def encode_document(document):\n",
    "    # TODO: use a pre-trained language model to encode the preprocessed document as a vector\n",
    "    return vector\n",
    "\n",
    "# encode each preprocessed document as a vector and add it to the Pinecone index\n",
    "for i, preprocessed_document in enumerate(preprocessed_documents):\n",
    "    vector = encode_document(preprocessed_document)\n",
    "    pinecone.upsert_index(index_name, [i], [vector])\n",
    "\n",
    "# define a function to search for documents containing a query\n",
    "def search_documents(query, n_results=10):\n",
    "    # preprocess the query\n",
    "    preprocessed_query = preprocess_document(query)\n",
    "    # encode the preprocessed query as a vector\n",
    "    query_vector = encode_document(preprocessed_query)\n",
    "    # search the Pinecone index for similar vectors\n",
    "    results = pinecone.query(index_name, [query_vector], top_k=n_results)\n",
    "    # return the indices of the top-k most similar documents\n",
    "    return results.ids[0]\n",
    "\n",
    "# example usage: search for documents containing the word \"lazy\"\n",
    "similar_documents = search_documents(\"lazy\")\n",
    "print(similar_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee73cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query):\n",
    "    # preprocess the query and encode it as a vector\n",
    "    preprocessed_query = preprocess_query(query)\n",
    "    query_vector = encode_query(preprocessed_query)\n",
    "\n",
    "    # perform the similarity search using Pinecone\n",
    "    search_results = pinecone_index.query(query_vector, top_k=10)\n",
    "\n",
    "    # return the most similar documents\n",
    "    similar_documents = []\n",
    "    for result in search_results:\n",
    "        document_index = result.id\n",
    "        document_score = result.score\n",
    "        similar_document = { \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A quick brown dog jumps over the lazy cat\",\n",
    "    \"The lazy dog is quick to bark but slow to bite\",\n",
    "    \"The cat is lazy and likes to sleep all day\"} # get the actual document from the index using the index id\n",
    "        similar_documents.append(similar_document)\n",
    "    return similar_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# initialize Pinecone client\n",
    "pinecone.init(api_key=\"YOUR_API_KEY\")\n",
    "\n",
    "# create a Pinecone index for text documents\n",
    "index_name = \"text_index\"\n",
    "pinecone.create_index(index_name, {\"metric\": \"cosine\"})\n",
    "\n",
    "# define a list of documents to index\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A quick brown dog jumps over the lazy cat\",\n",
    "    \"The lazy dog is quick to bark but slow to bite\",\n",
    "    \"The cat is lazy and likes to sleep all day\",\n",
    "]\n",
    "\n",
    "# define a function to preprocess a document (i.e., tokenize and remove stop words)\n",
    "def preprocess_document(document):\n",
    "    stop_words = {\"the\", \"a\", \"an\", \"is\", \"and\", \"but\", \"to\"}\n",
    "    tokens = document.lower().split()\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# preprocess each document and store the preprocessed documents in a list\n",
    "preprocessed_documents = [preprocess_document(document) for document in documents]\n",
    "\n",
    "# define a function to encode a preprocessed document as a vector\n",
    "def encode_document(document):\n",
    "    return vector\n",
    "\n",
    "# encode each preprocessed document as a vector and add it to the Pinecone index\n",
    "for i, preprocessed_document in enumerate(preprocessed_documents):\n",
    "    vector = encode_document(preprocessed_document)\n",
    "    pinecone.upsert_index(index_name, [i], [vector])\n",
    "\n",
    "# define a function to search for documents containing a query\n",
    "def search_documents(query, n_results=10):\n",
    "    # preprocess the query\n",
    "    preprocessed_query = preprocess_document(query)\n",
    "    # encode the preprocessed query as a vector\n",
    "    query_vector = encode_document(preprocessed_query)\n",
    "    # search the Pinecone index for similar vectors\n",
    "    results = pinecone.query(index_name, [query_vector], top_k=n_results)\n",
    "    # return the indices of the top-k most similar documents\n",
    "    return results.ids[0]\n",
    "\n",
    "# get user input\n",
    "user_query = input(\"Enter your search query: \")\n",
    "\n",
    "# encode user query into a vector\n",
    "query_vector = nlp(user_query).vector\n",
    "similar_documents = search_documents(user_query)\n",
    "print(similar_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "170eb358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\abhis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\abhis/nltk_data'\n    - 'C:\\\\python39\\\\nltk_data'\n    - 'C:\\\\python39\\\\share\\\\nltk_data'\n    - 'C:\\\\python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\abhis\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Remove stop words and lemmatize or stem remaining words\u001b[39;00m\n\u001b[0;32m     61\u001b[0m words \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m---> 62\u001b[0m words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, get_wordnet_pos(word)) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     63\u001b[0m words \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Join words back into a single string\u001b[39;00m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Remove stop words and lemmatize or stem remaining words\u001b[39;00m\n\u001b[0;32m     61\u001b[0m words \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[1;32m---> 62\u001b[0m words \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, \u001b[43mget_wordnet_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     63\u001b[0m words \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Join words back into a single string\u001b[39;00m\n",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36mget_wordnet_pos\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_wordnet_pos\u001b[39m(word):\n\u001b[1;32m---> 51\u001b[0m     tag \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m     52\u001b[0m     tag_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJ\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADJ,\n\u001b[0;32m     53\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mNOUN,\n\u001b[0;32m     54\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mVERB,\n\u001b[0;32m     55\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m\"\u001b[39m: wordnet\u001b[38;5;241m.\u001b[39mADV}\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tag_dict\u001b[38;5;241m.\u001b[39mget(tag, wordnet\u001b[38;5;241m.\u001b[39mNOUN)\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[1;32mC:\\python39\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\abhis/nltk_data'\n    - 'C:\\\\python39\\\\nltk_data'\n    - 'C:\\\\python39\\\\share\\\\nltk_data'\n    - 'C:\\\\python39\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\abhis\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from enchant.checker import SpellChecker\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import re\n",
    "#import polyglot\n",
    "#from polyglot.detect import Detector\n",
    "#from polyglot.text import Text\n",
    "#from pyxdameraulevenshtein import damerau_levenshtein_distance\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "documents = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"A quick brown dog jumps over the lazy cat\",\n",
    "    \"The lazy dog is quick to bark but slow to bite\",\n",
    "    \"The cat is lazy and likes to sleep all day\",\n",
    "]\n",
    "def preprocess_document(document):\n",
    "    stop_words = {\"the\", \"a\", \"an\", \"is\", \"and\", \"but\", \"to\"}\n",
    "    tokens = document.lower().split()\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# preprocess each document and store the preprocessed documents in a list\n",
    "preprocessed_documents = [preprocess_document(document) for document in documents]\n",
    "\n",
    "# Define paths to documents and stop words file\n",
    "DOCUMENTS_PATH = \"documents.txt\"\n",
    "STOPWORDS_PATH = \"stopwords.txt\"\n",
    "\n",
    "# Read documents from file\n",
    "with open('C:/Users/abhis/OneDrive/Desktop/AI semanitc search/documents.txt', \"r\") as f:\n",
    "    documents = f.readlines()\n",
    "\n",
    "# Read stop words from file\n",
    "with open('C:/Users/abhis/OneDrive/Desktop/AI semanitc search/stopwords.txt', \"r\") as f:\n",
    "    stop_words = set([line.strip() for line in f.readlines()])\n",
    "\n",
    "# Tokenize and preprocess documents\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "preprocessed_docs = []\n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "for doc in documents:\n",
    "    # Tokenize words\n",
    "    words = word_tokenize(doc)\n",
    "    # Remove stop words and lemmatize or stem remaining words\n",
    "    words = [word.lower() for word in words if not word in stop_words]\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    # Join words back into a single string\n",
    "    preprocessed_docs.append(' '.join(words))\n",
    "\n",
    "# Build TF-IDF vectorizer and calculate document similarity matrix\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_docs)\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Load spaCy model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Build dictionary and corpus for LDA topic modeling\n",
    "dictionary = Dictionary([doc.split() for doc in preprocessed_docs])\n",
    "corpus = [dictionary.doc2bow(doc.split()) for doc in preprocessed_docs]\n",
    "\n",
    "# Train LDA model and get topic probabilities for each document\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5)\n",
    "topic_probabilities = [lda_model.get_document_topics(corpus[i]) for i in range(len(corpus))]\n",
    "\n",
    "\n",
    "\n",
    "# Function to perform semantic search\n",
    "def semantic_search(query, documents, similarity_matrix, vectorizer, include_score=True, language='en', max_distance=1):\n",
    "    # Detect query language and translate if necessary\n",
    "    if language != 'en':\n",
    "        text = Text(query, hint_language_code=language)\n",
    "        query = text.translate('en').string.lower()\n",
    "    # Preprocess query\n",
    "    words = word_tokenize(query)\n",
    "    # Check spelling and correct if necessary\n",
    "    chkr = SpellChecker(\"en_US\")\n",
    "    for word in words:\n",
    "        chkr.set_text(word)\n",
    "        if not chkr.check():\n",
    "            for suggestion in chkr.suggest():\n",
    "                if suggestion in wordnet.words():\n",
    "                    query = query.replace(word, suggestion)\n",
    "                    break\n",
    "    # Remove stop words and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words if not word in stop_words]\n",
    "    # Expand query using synonyms\n",
    "    expanded_query = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc2547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
